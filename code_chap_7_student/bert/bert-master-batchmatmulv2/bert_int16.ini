[config]
quantization_type = int16
device_mode = clean
int_op_list = FC,BatchMatMul

[model]
activation_quantization_alg = naive
input_tensor_names = input_ids:0, input_mask:0, segment_ids:0
weight_quantization_alg = naive
seq_length = 128
original_models_path = ./squad_output_dir_128_small/frozen_model.pb
output_tensor_names = unstack:0,unstack:1
save_model_path = ./squad_output_dir_128_small/frozen_model_int16.pb

[data]
do_lower_case = False
doc_stride = 128
max_query_length = 64
batch_size = 8
num_runs = 1
vocab_file = bert/uncased_L-12_H-768_A-12/vocab.txt 
data_list = bert/squad/dev-v1.1.json

